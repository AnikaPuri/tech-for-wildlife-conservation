{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"UAVvideo.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"141nGZ2chSFT8jbNld_LoSDHv5mlohrf_","authorship_tag":"ABX9TyNZIObX86nu99dZDLZtFJEh"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"y0bEBiDQCT3W"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"Q7y-U5tqFO26"},"source":["# https://www.learnopencv.com/video-stabilization-using-point-feature-matching-in-opencv/"]},{"cell_type":"markdown","metadata":{"id":"y2JenSbOCXfr"},"source":["# Step 1 : Set Input and Output Videos\n"]},{"cell_type":"code","metadata":{"id":"HU-4vwY648mw","executionInfo":{"status":"ok","timestamp":1610329846326,"user_tz":300,"elapsed":1058,"user":{"displayName":"Anika Puri","photoUrl":"","userId":"03645349953561793544"}}},"source":["#pip uninstall opencv-python\n","#pip install opencv-python"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"e8fGYPta4_aW","executionInfo":{"status":"ok","timestamp":1610329852158,"user_tz":300,"elapsed":5161,"user":{"displayName":"Anika Puri","photoUrl":"","userId":"03645349953561793544"}},"outputId":"dcf285f5-1a4f-408c-8622-b3569b76589f"},"source":["# Import numpy and OpenCV\n","import numpy as np\n","import cv2\n","\n","# Read input video\n","#test video\n","#cap = cv2.VideoCapture('/content/drive/My Drive/video.mp4') \n","cap = cv2.VideoCapture('/content/drive/My Drive/wildlife1.mp4')\n","\n","# Get frame count\n","n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) \n","\n","# Get width and height of video stream\n","w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) \n","h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","print(w, h)\n","# Define the codec for output video\n","fourcc = cv2.VideoWriter_fourcc(*'XVID')\n","\n","fps = cap.get(cv2.CAP_PROP_FPS)\n","# Set up output video\n","out = cv2.VideoWriter('/content/drive/My Drive/wildlife1_out.mp4', fourcc, fps, (w, h))\n","\n","'''\n","for i in range(100):\n","  # Read next frame\n","  success, frame = cap.read() \n","  out.write(frame)\n","\n","cap.release()\n","out.release()\n","\n","# Closes all the frames\n","cv2.destroyAllWindows()\n","'''"],"execution_count":2,"outputs":[{"output_type":"stream","text":["698 332\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nfor i in range(100):\\n  # Read next frame\\n  success, frame = cap.read() \\n  out.write(frame)\\n\\ncap.release()\\nout.release()\\n\\n# Closes all the frames\\ncv2.destroyAllWindows()\\n'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"4Q7g3klrEIDM"},"source":["# Step 2: Read the first frame and convert it to grayscale"]},{"cell_type":"code","metadata":{"id":"8EhbTPRv5Ne9","executionInfo":{"status":"ok","timestamp":1610329859716,"user_tz":300,"elapsed":510,"user":{"displayName":"Anika Puri","photoUrl":"","userId":"03645349953561793544"}}},"source":["# Read first frame\n","_, prev = cap.read() \n","\n","# Convert frame to grayscale\n","prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY) "],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e5ywpMEFEPvE"},"source":["# Step 3: Find motion between frames\n","This is the most crucial part of the algorithm. We will iterate over all the frames, and find the motion between the current frame and the previous frame. It is not necessary to know the motion of each and every pixel. The Euclidean motion model requires that we know the motion of only 2 points in the two frames. However, in practice, it is a good idea to find the motion of 50-100 points, and then use them to robustly estimate the motion model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"7MqWJbqlfnGK","executionInfo":{"status":"ok","timestamp":1610329863226,"user_tz":300,"elapsed":631,"user":{"displayName":"Anika Puri","photoUrl":"","userId":"03645349953561793544"}},"outputId":"1e659d1a-6b31-4989-b9c2-3a6d304fc933"},"source":["'''\n","from matplotlib import pyplot as plt\n","img = cv2.imread('/content/drive/My Drive/track1.jpg')\n","gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n","corners = cv2.goodFeaturesToTrack(gray,1,0.01,30,blockSize=3)\n","corners = np.int0(corners)\n","for i in corners:\n","    x,y = i.ravel()\n","    cv2.circle(img,(x,y),3,255,-1)\n","plt.imshow(img),plt.show()\n","'''"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nfrom matplotlib import pyplot as plt\\nimg = cv2.imread('/content/drive/My Drive/track1.jpg')\\ngray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\\ncorners = cv2.goodFeaturesToTrack(gray,1,0.01,30,blockSize=3)\\ncorners = np.int0(corners)\\nfor i in corners:\\n    x,y = i.ravel()\\n    cv2.circle(img,(x,y),3,255,-1)\\nplt.imshow(img),plt.show()\\n\""]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"3LyjutP8Jtvz","executionInfo":{"status":"ok","timestamp":1610329867215,"user_tz":300,"elapsed":3045,"user":{"displayName":"Anika Puri","photoUrl":"","userId":"03645349953561793544"}}},"source":["# Pre-define transformation-store array\n","transforms = np.zeros((n_frames-1, 3), np.float32) \n","\n","for i in range(n_frames-2):\n","  # Detect feature points in previous frame\n","  prev_pts = cv2.goodFeaturesToTrack(prev_gray,\n","                                     maxCorners=200,\n","                                     qualityLevel=0.01,\n","                                     minDistance=30,\n","                                     blockSize=3)\n","   \n","  # Read next frame\n","  success, curr = cap.read() \n","  if not success: \n","    break \n","\n","  # Convert to grayscale\n","  curr_gray = cv2.cvtColor(curr, cv2.COLOR_BGR2GRAY) \n","\n","  # Calculate optical flow (i.e. track feature points)\n","  curr_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, prev_pts, None) \n","\n","  # Sanity check\n","  assert prev_pts.shape == curr_pts.shape \n","\n","  # Filter only valid points\n","  idx = np.where(status==1)[0]\n","  prev_pts = prev_pts[idx]\n","  curr_pts = curr_pts[idx]\n","\n","  #Find transformation matrix\n","  m, inliers = cv2.estimateAffinePartial2D(prev_pts, curr_pts)\n","  #m = cv2.estimateRigidTransform(prev_pts, curr_pts, fullAffine=False) #will only work with OpenCV-3 or less\n","   \n","  # Extract traslation\n","  dx = m[0,2]\n","  dy = m[1,2]\n","\n","  # Extract rotation angle\n","  da = np.arctan2(m[1,0], m[0,0])\n","   \n","  # Store transformation\n","  transforms[i] = [dx,dy,da]\n","   \n","  # Move to next frame\n","  prev_gray = curr_gray\n","\n","  #print(\"Frame: \" + str(i) +  \"/\" + str(n_frames) + \" -  Tracked points : \" + str(len(prev_pts)))"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jLe2JRJcJ_Qk"},"source":["# Step 4: Calculate smooth motion between frames\n","In the previous step, we estimated the motion between the frames and stored them in an array. We now need to find the trajectory of motion by cumulatively adding the differential motion estimated in the previous step."]},{"cell_type":"code","metadata":{"id":"S07D9A9XK-pY","executionInfo":{"status":"ok","timestamp":1610329869622,"user_tz":300,"elapsed":842,"user":{"displayName":"Anika Puri","photoUrl":"","userId":"03645349953561793544"}}},"source":["# Compute trajectory using cumulative sum of transformations\n","trajectory = np.cumsum(transforms, axis=0) "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WnPlyqlJ9R7","executionInfo":{"status":"ok","timestamp":1610329870900,"user_tz":300,"elapsed":685,"user":{"displayName":"Anika Puri","photoUrl":"","userId":"03645349953561793544"}}},"source":["SMOOTHING_RADIUS = 50\n","\n","def movingAverage(curve, radius): \n","  window_size = 2 * radius + 1\n","  # Define the filter \n","  f = np.ones(window_size)/window_size \n","  # Add padding to the boundaries \n","  curve_pad = np.lib.pad(curve, (radius, radius), 'edge') \n","  # Apply convolution \n","  curve_smoothed = np.convolve(curve_pad, f, mode='same') \n","  # Remove padding \n","  curve_smoothed = curve_smoothed[radius:-radius]\n","  # return smoothed curve\n","  return curve_smoothed \n","\n","def smooth(trajectory): \n","  smoothed_trajectory = np.copy(trajectory) \n","  # Filter the x, y and angle curves\n","  for i in range(3):\n","    smoothed_trajectory[:,i] = movingAverage(trajectory[:,i], radius=SMOOTHING_RADIUS)\n","\n","  return smoothed_trajectory"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fLnpVOkK6jn","executionInfo":{"status":"ok","timestamp":1610329871750,"user_tz":300,"elapsed":480,"user":{"displayName":"Anika Puri","photoUrl":"","userId":"03645349953561793544"}}},"source":["# Calculate difference in smoothed_trajectory and trajectory\n","smoothed_trajectory = smooth(trajectory)\n","difference = smoothed_trajectory - trajectory\n"," \n","# Calculate newer transformation array\n","transforms_smooth = transforms + difference"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fzSPy5bSM5ky"},"source":["# Step 5: Apply smoothed camera motion to frames"]},{"cell_type":"code","metadata":{"id":"X0oHN0T86Rhm","executionInfo":{"status":"ok","timestamp":1610329874801,"user_tz":300,"elapsed":622,"user":{"displayName":"Anika Puri","photoUrl":"","userId":"03645349953561793544"}}},"source":["def fixBorder(frame):\n","  s = frame.shape\n","  # Scale the image 4% without moving the center\n","  T = cv2.getRotationMatrix2D((s[1]/2, s[0]/2), 0, 1.04)\n","  frame = cv2.warpAffine(frame, T, (s[1], s[0]))\n","  return frame"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"fES7W4pwNAya","executionInfo":{"status":"ok","timestamp":1610329877575,"user_tz":300,"elapsed":2259,"user":{"displayName":"Anika Puri","photoUrl":"","userId":"03645349953561793544"}}},"source":["# Reset stream to first frame \n","cap.set(cv2.CAP_PROP_POS_FRAMES, 0) \n","\n","from google.colab.patches import cv2_imshow\n"," \n","\n","# Write n_frames-1 transformed frames\n","for i in range(n_frames-2):\n","  # Read next frame\n","  success, frame = cap.read() \n","  if not success:\n","    break\n","\n","  # Extract transformations from the new transformation array\n","  dx = transforms_smooth[i,0]\n","  dy = transforms_smooth[i,1]\n","  da = transforms_smooth[i,2]\n","\n","  # Reconstruct transformation matrix accordingly to new values\n","  m = np.zeros((2,3), np.float32)\n","  m[0,0] = np.cos(da)\n","  m[0,1] = -np.sin(da)\n","  m[1,0] = np.sin(da)\n","  m[1,1] = np.cos(da)\n","  m[0,2] = dx\n","  m[1,2] = dy\n","\n","  # Apply affine wrapping to the given frame\n","  frame_stabilized = cv2.warpAffine(frame, m, (w,h))\n","\n","  # Fix border artifacts\n","  frame_stabilized = fixBorder(frame_stabilized) \n","\n","  '''\n","  # Write the frame to the file\n","  frame_out = cv2.hconcat([frame, frame_stabilized])\n","\n","  # If the image is too big, resize it.\n","  if(frame_out.shape[1] > 1920): \n","    frame_out = cv2.resize(frame_out, (frame_out.shape[1]/2, frame_out.shape[0]/2))\n","  '''\n","\n","  #cv2_imshow(frame_stabilized)\n","  cv2.waitKey(10)\n","  out.write(frame_stabilized)\n","\n","\n","cap.release()\n","out.release()\n","\n","# Closes all the frames\n","cv2.destroyAllWindows() \n","\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"gW0x30KVhScE","executionInfo":{"status":"ok","timestamp":1610329879091,"user_tz":300,"elapsed":410,"user":{"displayName":"Anika Puri","photoUrl":"","userId":"03645349953561793544"}}},"source":[""],"execution_count":10,"outputs":[]}]}