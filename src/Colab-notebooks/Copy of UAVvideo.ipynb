{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of UAVvideo.ipynb","provenance":[{"file_id":"141nGZ2chSFT8jbNld_LoSDHv5mlohrf_","timestamp":1609016945029}],"collapsed_sections":[],"mount_file_id":"141nGZ2chSFT8jbNld_LoSDHv5mlohrf_","authorship_tag":"ABX9TyNEJmQ5ANfSXL8YD/bIdSLq"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"y0bEBiDQCT3W"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"Q7y-U5tqFO26"},"source":["# https://www.learnopencv.com/video-stabilization-using-point-feature-matching-in-opencv/"]},{"cell_type":"markdown","metadata":{"id":"y2JenSbOCXfr"},"source":["# Step 1 : Set Input and Output Videos\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":232},"id":"e8fGYPta4_aW","executionInfo":{"status":"error","timestamp":1609016882477,"user_tz":300,"elapsed":272,"user":{"displayName":"Anika Puri","photoUrl":"","userId":"03645349953561793544"}},"outputId":"f1f8406f-32de-4931-9e29-a6a4ecfd9a57"},"source":["# Import numpy and OpenCV\n","import numpy as np\n","import cv2\n","\n","# Read input video\n","cap = cv2.VideoCapture('video.mp4')&nbsp;\n","\n","# Get frame count\n","n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) \n","\n","# Get width and height of video stream\n","w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) \n","h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","# Define the codec for output video\n","fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n","\n","# Set up output video\n","out = cv2.VideoWriter('video_out.mp4', fourcc, fps, (w, h))"],"execution_count":6,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-ec9f8b407c97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Read input video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'video.mp4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0mnbsp\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Get frame count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nbsp' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"4Q7g3klrEIDM"},"source":["# Step 2: Read the first frame and convert it to grayscale"]},{"cell_type":"code","metadata":{"id":"8EhbTPRv5Ne9"},"source":["# Read first frame\n","_, prev = cap.read() \n","\n","# Convert frame to grayscale\n","prev_gray = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e5ywpMEFEPvE"},"source":["# Step 3: Find motion between frames\n","This is the most crucial part of the algorithm. We will iterate over all the frames, and find the motion between the current frame and the previous frame. It is not necessary to know the motion of each and every pixel. The Euclidean motion model requires that we know the motion of only 2 points in the two frames. However, in practice, it is a good idea to find the motion of 50-100 points, and then use them to robustly estimate the motion model."]},{"cell_type":"code","metadata":{"id":"3LyjutP8Jtvz"},"source":["# Pre-define transformation-store array\n","transforms = np.zeros((n_frames-1, 3), np.float32) \n","\n","for i in range(n_frames-2):\n","  # Detect feature points in previous frame\n","  prev_pts = cv2.goodFeaturesToTrack(prev_gray,\n","                                     maxCorners=200,\n","                                     qualityLevel=0.01,\n","                                     minDistance=30,\n","                                     blockSize=3)\n","   \n","  # Read next frame\n","  success, curr = cap.read() \n","  if not success: \n","    break \n","\n","  # Convert to grayscale\n","  curr_gray = cv2.cvtColor(curr, cv2.COLOR_BGR2GRAY) \n","\n","  # Calculate optical flow (i.e. track feature points)\n","  curr_pts, status, err = cv2.calcOpticalFlowPyrLK(prev_gray, curr_gray, prev_pts, None) \n","\n","  # Sanity check\n","  assert prev_pts.shape == curr_pts.shape \n","\n","  # Filter only valid points\n","  idx = np.where(status==1)[0]\n","  prev_pts = prev_pts[idx]\n","  curr_pts = curr_pts[idx]\n","\n","  #Find transformation matrix\n","  m = cv2.estimateRigidTransform(prev_pts, curr_pts, fullAffine=False) #will only work with OpenCV-3 or less\n","   \n","  # Extract traslation\n","  dx = m[0,2]\n","  dy = m[1,2]\n","\n","  # Extract rotation angle\n","  da = np.arctan2(m[1,0], m[0,0])\n","   \n","  # Store transformation\n","  transforms[i] = [dx,dy,da]\n","   \n","  # Move to next frame\n","  prev_gray = curr_gray\n","\n","  print(\"Frame: \" + str(i) +  \"/\" + str(n_frames) + \" -  Tracked points : \" + str(len(prev_pts)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jLe2JRJcJ_Qk"},"source":["# Step 4: Calculate smooth motion between frames\n","In the previous step, we estimated the motion between the frames and stored them in an array. We now need to find the trajectory of motion by cumulatively adding the differential motion estimated in the previous step."]},{"cell_type":"code","metadata":{"id":"S07D9A9XK-pY"},"source":["# Compute trajectory using cumulative sum of transformations\n","trajectory = np.cumsum(transforms, axis=0) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WnPlyqlJ9R7"},"source":["def movingAverage(curve, radius): \n","  window_size = 2 * radius + 1\n","  # Define the filter \n","  f = np.ones(window_size)/window_size \n","  # Add padding to the boundaries \n","  curve_pad = np.lib.pad(curve, (radius, radius), 'edge') \n","  # Apply convolution \n","  curve_smoothed = np.convolve(curve_pad, f, mode='same') \n","  # Remove padding \n","  curve_smoothed = curve_smoothed[radius:-radius]\n","  # return smoothed curve\n","  return curve_smoothed \n","\n","def smooth(trajectory): \n","  smoothed_trajectory = np.copy(trajectory) \n","  # Filter the x, y and angle curves\n","  for i in range(3):\n","    smoothed_trajectory[:,i] = movingAverage(trajectory[:,i], radius=SMOOTHING_RADIUS)\n","\n","  return smoothed_trajectory"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fLnpVOkK6jn"},"source":["# Calculate difference in smoothed_trajectory and trajectory\n","difference = smoothed_trajectory - trajectory\n"," \n","# Calculate newer transformation array\n","transforms_smooth = transforms + difference"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fzSPy5bSM5ky"},"source":["# Step 5: Apply smoothed camera motion to frames"]},{"cell_type":"code","metadata":{"id":"fES7W4pwNAya"},"source":["# Reset stream to first frame \n","cap.set(cv2.CAP_PROP_POS_FRAMES, 0) \n"," \n","# Write n_frames-1 transformed frames\n","for i in range(n_frames-2):\n","  # Read next frame\n","  success, frame = cap.read() \n","  if not success:\n","    break\n","\n","  # Extract transformations from the new transformation array\n","  dx = transforms_smooth[i,0]\n","  dy = transforms_smooth[i,1]\n","  da = transforms_smooth[i,2]\n","\n","  # Reconstruct transformation matrix accordingly to new values\n","  m = np.zeros((2,3), np.float32)\n","  m[0,0] = np.cos(da)\n","  m[0,1] = -np.sin(da)\n","  m[1,0] = np.sin(da)\n","  m[1,1] = np.cos(da)\n","  m[0,2] = dx\n","  m[1,2] = dy\n","\n","  # Apply affine wrapping to the given frame\n","  frame_stabilized = cv2.warpAffine(frame, m, (w,h))\n","\n","  # Fix border artifacts\n","  frame_stabilized = fixBorder(frame_stabilized) \n","\n","  # Write the frame to the file\n","  frame_out = cv2.hconcat([frame, frame_stabilized])\n","\n","  # If the image is too big, resize it.\n","  if(frame_out.shape[1] &gt; 1920): \n","    frame_out = cv2.resize(frame_out, (frame_out.shape[1]/2, frame_out.shape[0]/2));\n","  \n","  cv2.imshow(\"Before and After\", frame_out)\n","  cv2.waitKey(10)\n","  out.write(frame_out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCl07yZNNH-6"},"source":["def fixBorder(frame):\n","  s = frame.shape\n","  # Scale the image 4% without moving the center\n","  T = cv2.getRotationMatrix2D((s[1]/2, s[0]/2), 0, 1.04)\n","  frame = cv2.warpAffine(frame, T, (s[1], s[0]))\n","  return frame"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5S0Ih5oNJKm"},"source":[""],"execution_count":null,"outputs":[]}]}